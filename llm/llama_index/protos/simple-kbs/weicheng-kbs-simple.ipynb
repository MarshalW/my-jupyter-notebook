{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5793a0fb-2afb-4836-97de-98ac2b3e297f",
   "metadata": {},
   "source": [
    "# 围城数据的 KBs - 简化版"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4c107b-f720-44fe-bf7b-59c3975a2725",
   "metadata": {},
   "source": [
    "结论：\n",
    "\n",
    "- qdrant 有2个 client，同步和异步的\n",
    "- llamaindex 使用了二者\n",
    "- 如果使用qdrant，需要使用网络客户端？（本地存储是否可以？）\n",
    "- bm25+embedding组合不适合长篇小说找到正确答案\n",
    "- 成功的是\n",
    "    - 验证了bm25+embedding组合技术可行性\n",
    "    - qdrant 使用 async client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3338eaa1-58bc-4c9d-8a95-7a7bba915f05",
   "metadata": {},
   "source": [
    "## 生成数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9064a9ea-68cf-4867-adbf-2cb937dc3896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.66 s, sys: 417 ms, total: 3.08 s\n",
      "Wall time: 2.73 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents=SimpleDirectoryReader(input_files=[\"books/围城.txt\"]).load_data()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06c56d8-76ff-46e3-9af9-1f525f3f8bd2",
   "metadata": {},
   "source": [
    "## 设置 LlamaIndex 全局变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06f6d186-34d4-4f82-9239-306856801d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 673 ms, sys: 40.2 ms, total: 714 ms\n",
      "Wall time: 823 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 加载llm和embeddings\n",
    "%run ../../utils2.py\n",
    "\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm=get_llm(model=\"qwen2-7b-6k\")\n",
    "\n",
    "# Settings.embed_model = get_embedding(model_name=\"quentinz/bge-large-zh-v1.5\")\n",
    "# embedding_dimension=1024\n",
    "\n",
    "Settings.embed_model = get_embedding(model_name=\"rjmalagon/gte-qwen2-1.5b-instruct-embed-f16\")\n",
    "embedding_dimension=1536\n",
    "\n",
    "persist_dir=\"/tmp/weicheng-kbs-simple\"\n",
    "!rm -rf $persist_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da44270c-95aa-4799-98a8-f787b2e4bc1a",
   "metadata": {},
   "source": [
    "## 设置向量存储 - Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "069a307a-fce8-4c4c-8af8-87381e8f78cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 100 ms, sys: 0 ns, total: 100 ms\n",
      "Wall time: 210 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from qdrant_client import QdrantClient, AsyncQdrantClient\n",
    "from qdrant_client import models\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# client = QdrantClient(\":memory:\")\n",
    "# aclient = AsyncQdrantClient(\":memory:\")\n",
    "client = QdrantClient(\"http://ape:6333\")\n",
    "aclient = AsyncQdrantClient(\"http://ape:6333\")\n",
    "collection_name=\"weicheng-simple\"\n",
    "\n",
    "if not client.collection_exists(collection_name):\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=embedding_dimension,\n",
    "            distance=models.Distance.COSINE,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c62128b-6ff2-45d3-8fb7-ea6457a3a907",
   "metadata": {},
   "source": [
    "## 创建向量存储索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e568b18-db33-40b6-96f4-6eb526ca1cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 803 ms, sys: 28.1 ms, total: 831 ms\n",
      "Wall time: 833 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3006"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from qdrant_client import models\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client, \n",
    "    aclient=aclient,\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=embedding_dimension,\n",
    "        distance=models.Distance.COSINE,\n",
    "    ),\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store,\n",
    ")\n",
    "\n",
    "parser = SentenceSplitter(\n",
    "    chunk_size=128,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "document_nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "len(document_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6245a868-00b2-47b9-99b7-baaf1bd34713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.7 s, sys: 378 ms, total: 18.1 s\n",
      "Wall time: 5min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "index = VectorStoreIndex(document_nodes, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750aabae-a16c-4e84-8ef3-9592c43b794f",
   "metadata": {},
   "source": [
    "## 基于嵌入检索的查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f1842f5-5a7f-404a-9f20-6b323cf62ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据提供的信息，无法确定方鸿渐的妻子是谁。文中提到了“苏文纨小姐”，但并未明确表示她就是方鸿渐的配偶。因此，不能仅凭这些信息推断出方鸿渐的妻子身份。CPU times: user 165 ms, sys: 8.83 ms, total: 174 ms\n",
      "Wall time: 4.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "embedding_retriever = index.as_retriever(\n",
    "    similarity_top_k=5,\n",
    ")\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=embedding_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "# query\n",
    "streaming_response = query_engine.query(\"方鸿渐的老婆是谁\")\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987f67e4-e2d0-4bab-921e-ad1cc2251fd8",
   "metadata": {},
   "source": [
    "## 基于 BM25 检索的查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b340b98b-11d6-40ec-b0e7-70db9b899a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 ms, sys: 77 µs, total: 11.3 ms\n",
      "Wall time: 412 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 下载停用词\n",
    "\n",
    "# 设置 HTTP 代理环境变量\n",
    "# https://github.com/nltk/nltk_data/issues/154#issuecomment-2144880495\n",
    "http_proxy=\"http://192.168.0.134:7890\"\n",
    "\n",
    "import nltk\n",
    "nltk.set_proxy(f'{http_proxy}')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8222cce8-18fc-4919-85f4-ffb2d4565db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 61.5 ms, sys: 13.5 ms, total: 75.1 ms\n",
      "Wall time: 75.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import jieba\n",
    "from typing import List\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def chinese_tokenizer(text: str) -> List[str]:\n",
    "    # Use jieba to segment Chinese text\n",
    "    return list(jieba.cut(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b279e6b-100b-41c3-86a6-ac8b08ace821",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.413 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.46 s, sys: 39.5 ms, total: 2.5 s\n",
      "Wall time: 2.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=128, chunk_overlap=10)\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(splitter.get_nodes_from_documents(documents))\n",
    "\n",
    "bm25_retriever=BM25Retriever.from_defaults(\n",
    "    docstore=docstore, \n",
    "    similarity_top_k=5,\n",
    "    tokenizer=chinese_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb2cf8b3-b35f-445a-80e0-49ba0ed23fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据提供的信息，无法直接确定方鸿渐的老婆具体是哪位角色。在给定的文本片段中，并没有明确提到方鸿渐的具体妻子名字。需要更多的上下文或详细信息来准确回答这个问题。CPU times: user 94.9 ms, sys: 8.59 ms, total: 103 ms\n",
      "Wall time: 1.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=bm25_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "# query\n",
    "streaming_response = query_engine.query(\"方鸿渐的老婆是谁\")\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05081c8b-9e02-4e7e-9f18-3d139c688bee",
   "metadata": {},
   "source": [
    "## 基于混合检索的查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bf03a1d-5bcc-4c04-9107-a49284d48815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 3.27 ms, total: 3.27 ms\n",
      "Wall time: 2.71 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "\n",
    "fusion_retriever = QueryFusionRetriever(\n",
    "    [\n",
    "        embedding_retriever,\n",
    "        bm25_retriever,\n",
    "    ],\n",
    "    num_queries=1,\n",
    "    use_async=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "743ed6d7-8880-4d30-824f-00ffcff8ade3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据提供的信息，方鸿渐的妻子是中国的一位白俄女性。她是在中国与方鸿渐结婚的，并且他们共同生活了二十多年，期间生了一个儿子并且该儿子已经大学毕业。不幸的是，这位妻子早逝。CPU times: user 97.6 ms, sys: 13.9 ms, total: 112 ms\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "# query_engine = RetrieverQueryEngine.from_args(fusion_retriever)\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=fusion_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "# query\n",
    "streaming_response = query_engine.query(\"方鸿渐的老婆是谁\")\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c228f-3d69-4fc1-8983-41cdc8c86490",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
