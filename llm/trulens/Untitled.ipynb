{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d18bcb7-f826-426e-ae7a-fc4320364b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.2 ms, sys: 8.99 ms, total: 22.2 ms\n",
      "Wall time: 2.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture\n",
    "\n",
    "!pip install trulens_eval llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc1f9e7-715a-4b79-8564-7d37f03e3248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
      "ğŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n",
      "CPU times: user 6.4 s, sys: 496 ms, total: 6.89 s\n",
      "Wall time: 6.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import nltk\n",
    "nltk.set_proxy('http://myproxy:7890')\n",
    "\n",
    "from trulens_eval import Tru\n",
    "tru = Tru()\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4562be-00f2-4d51-bbe3-ec1ca297ea46",
   "metadata": {},
   "source": [
    "## æœ€åŸºæœ¬çš„æ£€ç´¢æŸ¥è¯¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeac5bc9-a919-4977-b239-86654cb8e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 416 ms, sys: 20.4 ms, total: 436 ms\n",
      "Wall time: 8.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "Settings.chunk_size = 128\n",
    "Settings.chunk_overlap = 16\n",
    "\n",
    "Settings.llm = OpenAILike(\n",
    "    model=\"qwen2\", \n",
    "    api_base=\"http://192.168.0.73:11434/v1\", \n",
    "    api_key=\"ollama\",\n",
    "    is_chat_model=True,\n",
    "    temperature=0.1,\n",
    "    request_timeout=60.0\n",
    ")\n",
    "\n",
    "Settings.embed_model =OllamaEmbedding(\n",
    "    model_name=\"quentinz/bge-large-zh-v1.5\",\n",
    "    base_url=\"http://monkey:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0}, # -mirostat N ä½¿ç”¨ Mirostat é‡‡æ ·ã€‚\n",
    ")\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5602c1c0-b7ec-4ed9-9183-1f6debf68aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŒ—æµ·å…¬å›­ä½äºåŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·ã€‚\n",
      "CPU times: user 64.8 ms, sys: 6 Âµs, total: 64.8 ms\n",
      "Wall time: 664 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "response = query_engine.query(\"åŒ—æµ·å…¬å›­åœ¨å“ªé‡Œï¼Ÿ\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ca36a0-63bc-4d69-b18b-4dd3f9a63d7d",
   "metadata": {},
   "source": [
    "## è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "221ea483-408f-43f0-96f4-b9a19d409432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.53 ms, sys: 8.64 ms, total: 16.2 ms\n",
      "Wall time: 1.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture\n",
    "\n",
    "!pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abd020b9-b108-40dd-8f6e-6b72cc4ce8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56 ms, sys: 8.05 ms, total: 64.1 ms\n",
      "Wall time: 63.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import litellm\n",
    "\n",
    "from trulens_eval.feedback.provider import LiteLLM\n",
    "\n",
    "# litellm.set_verbose = False\n",
    "\n",
    "ollama_provider = LiteLLM(\n",
    "    model_engine=\"ollama/qwen2\", \n",
    "    api_base=\"http://192.168.0.73:11434\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33ade74e-4c84-4ed9-939b-1401b32d5971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Groundedness, input source will be set to __record__.app.query.rets.source_nodes[:].node.text.collect() .\n",
      "âœ… In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "âœ… In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "âœ… In Context Relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Context Relevance, input context will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
      "CPU times: user 29.1 ms, sys: 67 Âµs, total: 29.2 ms\n",
      "Wall time: 29.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from trulens_eval.feedback.provider import OpenAI\n",
    "from trulens_eval import Feedback\n",
    "import numpy as np\n",
    "\n",
    "# Initialize provider class\n",
    "provider = ollama_provider\n",
    "\n",
    "# select context to be used in feedback. the location of context is app specific.\n",
    "from trulens_eval.app import App\n",
    "context = App.select_context(query_engine)\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(provider.groundedness_measure_with_cot_reasons, name = \"Groundedness\")\n",
    "    .on(context.collect()) # collect context chunks into a list\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_answer_relevance = (\n",
    "    Feedback(provider.relevance_with_cot_reasons, name = \"Answer Relevance\")\n",
    "    .on_input_output()\n",
    ")\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(provider.context_relevance_with_cot_reasons, name = \"Context Relevance\")\n",
    "    .on_input()\n",
    "    .on(context)\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87098d4e-5a61-4bf6-a2e0-69b7187acb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 225 ms, sys: 7.3 ms, total: 233 ms\n",
      "Wall time: 243 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from trulens_eval import TruLlama\n",
    "tru_query_engine_recorder = TruLlama(\n",
    "    query_engine,\n",
    "    app_id='LlamaIndex_App1',\n",
    "    feedbacks=[f_groundedness, f_answer_relevance, f_context_relevance]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "802b31ad-56b4-46d0-b916-98b7b61ac2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(temperature=0.0, model='ollama/qwen2', messages=[{'role': 'system', 'content': \"You are a RELEVANCE grader; providing the relevance of the given RESPONSE to the given PROMPT.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long RESPONSES should score equally well as short RESPONSES.\\n\\n        - Answers that intentionally do not answer the question, such as 'I don't know' and model refusals, should also be counted as the most RELEVANT.\\n\\n        - RESPONSE must be relevant to the entire PROMPT to get a score of 10.\\n\\n        - RELEVANCE score should increase as the RESPONSE provides RELEVANT context to more parts of the PROMPT.\\n\\n        - RESPONSE that is RELEVANT to none of the PROMPT should get a score of 0.\\n\\n        - RESPONSE that is RELEVANT to some of the PROMPT should get as score of 2, 3, or 4. Higher score indicates more RELEVANCE.\\n\\n        - RESPONSE that is RELEVANT to most of the PROMPT should get a score between a 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - RESPONSE that is RELEVANT to the entire PROMPT should get a score of 9 or 10.\\n\\n        - RESPONSE that is RELEVANT and answers the entire PROMPT completely should get a score of 10.\\n\\n        - RESPONSE that confidently FALSE should get a score of 0.\\n\\n        - RESPONSE that is only seemingly RELEVANT should get a score of 0.\\n\\n        - Never elaborate.\\n        \"}, {'role': 'user', 'content': 'PROMPT: åŒ—æµ·å…¬å›­åœ¨å“ªé‡Œï¼Ÿ\\n\\n        RESPONSE: åŒ—æµ·å…¬å›­ä½äºåŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·ã€‚\\n\\n        \\nPlease answer using the entire template below.\\n\\nTEMPLATE: \\nScore: <The score 0-10 based on the given criteria>\\nCriteria: <Provide the criteria for this evaluation>\\nSupporting Evidence: <Provide your reasons for scoring based on the listed criteria step by step. Tie it back to the evaluation being completed.>\\n '}])\u001b[0m\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0.0}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'qwen2', 'prompt': \"### System:\\nYou are a RELEVANCE grader; providing the relevance of the given RESPONSE to the given PROMPT.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long RESPONSES should score equally well as short RESPONSES.\\n\\n        - Answers that intentionally do not answer the question, such as 'I don't know' and model refusals, should also be counted as the most RELEVANT.\\n\\n        - RESPONSE must be relevant to the entire PROMPT to get a score of 10.\\n\\n        - RELEVANCE score should increase as the RESPONSE provides RELEVANT context to more parts of the PROMPT.\\n\\n        - RESPONSE that is RELEVANT to none of the PROMPT should get a score of 0.\\n\\n        - RESPONSE that is RELEVANT to some of the PROMPT should get as score of 2, 3, or 4. Higher score indicates more RELEVANCE.\\n\\n        - RESPONSE that is RELEVANT to most of the PROMPT should get a score between a 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - RESPONSE that is RELEVANT to the entire PROMPT should get a score of 9 or 10.\\n\\n        - RESPONSE that is RELEVANT and answers the entire PROMPT completely should get a score of 10.\\n\\n        - RESPONSE that confidently FALSE should get a score of 0.\\n\\n        - RESPONSE that is only seemingly RELEVANT should get a score of 0.\\n\\n        - Never elaborate.\\n        \\n\\n### User:\\nPROMPT: åŒ—æµ·å…¬å›­åœ¨å“ªé‡Œï¼Ÿ\\n\\n        RESPONSE: åŒ—æµ·å…¬å›­ä½äºåŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·ã€‚\\n\\n        \\nPlease answer using the entire template below.\\n\\nTEMPLATE: \\nScore: <The score 0-10 based on the given criteria>\\nCriteria: <Provide the criteria for this evaluation>\\nSupporting Evidence: <Provide your reasons for scoring based on the listed criteria step by step. Tie it back to the evaluation being completed.>\\n \\n\\n\", 'options': {'temperature': 0.0}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "CPU times: user 1.84 s, sys: 31.2 ms, total: 1.87 s\n",
      "Wall time: 4.36 s\n",
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(temperature=0.0, model='ollama/qwen2', messages=[{'role': 'system', 'content': 'You are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.'}, {'role': 'user', 'content': 'QUESTION: åŒ—æµ·å…¬å›­åœ¨å“ªé‡Œï¼Ÿ\\n\\n        CONTEXT: Beihai Park\\n- åœ°ç†ä½ç½®\\n- åŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·\\n- æ°”å€™æ¡ä»¶\\n- æ¸©å¸¦å­£é£æ°”å€™\\n- å¼€æ”¾æ—¶é—´\\n- 4æœˆ1æ—¥-10æœˆ31æ—¥ 06:00-21:00ï¼›11æœˆ1æ—¥-æ¬¡å¹´3æœˆ31æ—¥ 06:30-20:00 [1]\\n-\\n        \\n        \\nPlease answer using the entire template below.\\n\\nTEMPLATE: \\nScore: <The score 0-10 based on the given criteria>\\nCriteria: <Provide the criteria for this evaluation>\\nSupporting Evidence: <Provide your reasons for scoring based on the listed criteria step by step. Tie it back to the evaluation being completed.>\\n '}])\u001b[0m\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0.0}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'qwen2', 'prompt': '### System:\\nYou are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.\\n\\n### User:\\nQUESTION: åŒ—æµ·å…¬å›­åœ¨å“ªé‡Œï¼Ÿ\\n\\n        CONTEXT: Beihai Park\\n- åœ°ç†ä½ç½®\\n- åŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·\\n- æ°”å€™æ¡ä»¶\\n- æ¸©å¸¦å­£é£æ°”å€™\\n- å¼€æ”¾æ—¶é—´\\n- 4æœˆ1æ—¥-10æœˆ31æ—¥ 06:00-21:00ï¼›11æœˆ1æ—¥-æ¬¡å¹´3æœˆ31æ—¥ 06:30-20:00 [1]\\n-\\n        \\n        \\nPlease answer using the entire template below.\\n\\nTEMPLATE: \\nScore: <The score 0-10 based on the given criteria>\\nCriteria: <Provide the criteria for this evaluation>\\nSupporting Evidence: <Provide your reasons for scoring based on the listed criteria step by step. Tie it back to the evaluation being completed.>\\n \\n\\n', 'options': {'temperature': 0.0}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(temperature=0.0, model='ollama/qwen2', messages=[{'role': 'system', 'content': 'You are a INFORMATION OVERLAP classifier; providing the overlap of information between the source and statement.\\n        Respond only as a number from 0 to 10 where 0 is no information overlap and 10 is all information is overlapping.\\n        Never elaborate.'}, {'role': 'user', 'content': \"SOURCE: ['Beihai Park\\\\n- åœ°ç†ä½ç½®\\\\n- åŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·\\\\n- æ°”å€™æ¡ä»¶\\\\n- æ¸©å¸¦å­£é£æ°”å€™\\\\n- å¼€æ”¾æ—¶é—´\\\\n- 4æœˆ1æ—¥-10æœˆ31æ—¥ 06:00-21:00ï¼›11æœˆ1æ—¥-æ¬¡å¹´3æœˆ31æ—¥ 06:30-20:00 [1]\\\\n-', '[32]åŒ—æµ·å…¬å›­ä¹Ÿæ˜¯å›½å®¶AAAAçº§æ—…æ¸¸æ™¯åŒºã€‚ [33]\\\\n2023å¹´4æœˆï¼ŒåŒ—æµ·å…¬å›­å…¥é€‰ç¬¬äºŒæ‰¹å›½å®¶çº§æ–‡æ˜æ—…æ¸¸ç¤ºèŒƒå•ä½å…¬ç¤ºåå•ã€‚ [37]\\\\n- ä¸­æ–‡å\\\\n- åŒ—æµ·å…¬å›­\\\\n- å¤–æ–‡å\\\\n- Beihai Park\\\\n- åœ°ç†ä½ç½®\\\\n-', '[1]å…¨å›­ä»¥åŒ—æµ·ä¸ºä¸­å¿ƒï¼Œå…¨å›­å åœ°69å…¬é¡·ï¼ˆå…¶ä¸­æ°´é¢39å…¬é¡·ï¼‰ï¼Œä¸»è¦ç”±ç¼åå²›ã€ä¸œå²¸å’ŒåŒ—å²¸æ™¯åŒºç»„æˆã€‚ç¼åå²›ä¸Šæ ‘æœ¨è‹éƒï¼Œäº­å°æ¥¼é˜å¹½é™ï¼Œç™½å¡”è€¸ç«‹å±±å·…ï¼Œæˆä¸ºå…¬å›­çš„æ ‡å¿—ã€‚']\\n        \\n        Hypothesis: åŒ—æµ·å…¬å›­ä½äºåŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·ã€‚\\n        \\n        Please answer with the template below for all statement sentences:\\n\\n        Criteria: <Statement Sentence>, \\n        Supporting Evidence: <Identify and describe the location in the source where the information matches the statement. Provide a detailed, human-readable summary indicating the path or key details. if nothing matches, say NOTHING FOUND>\\n        Score: <Output a number between 0-10 where 0 is no information overlap and 10 is all information is overlapping>\\n        \"}])\u001b[0m\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0.0}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'qwen2', 'prompt': \"### System:\\nYou are a INFORMATION OVERLAP classifier; providing the overlap of information between the source and statement.\\n        Respond only as a number from 0 to 10 where 0 is no information overlap and 10 is all information is overlapping.\\n        Never elaborate.\\n\\n### User:\\nSOURCE: ['Beihai Park\\\\n- åœ°ç†ä½ç½®\\\\n- åŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·\\\\n- æ°”å€™æ¡ä»¶\\\\n- æ¸©å¸¦å­£é£æ°”å€™\\\\n- å¼€æ”¾æ—¶é—´\\\\n- 4æœˆ1æ—¥-10æœˆ31æ—¥ 06:00-21:00ï¼›11æœˆ1æ—¥-æ¬¡å¹´3æœˆ31æ—¥ 06:30-20:00 [1]\\\\n-', '[32]åŒ—æµ·å…¬å›­ä¹Ÿæ˜¯å›½å®¶AAAAçº§æ—…æ¸¸æ™¯åŒºã€‚ [33]\\\\n2023å¹´4æœˆï¼ŒåŒ—æµ·å…¬å›­å…¥é€‰ç¬¬äºŒæ‰¹å›½å®¶çº§æ–‡æ˜æ—…æ¸¸ç¤ºèŒƒå•ä½å…¬ç¤ºåå•ã€‚ [37]\\\\n- ä¸­æ–‡å\\\\n- åŒ—æµ·å…¬å›­\\\\n- å¤–æ–‡å\\\\n- Beihai Park\\\\n- åœ°ç†ä½ç½®\\\\n-', '[1]å…¨å›­ä»¥åŒ—æµ·ä¸ºä¸­å¿ƒï¼Œå…¨å›­å åœ°69å…¬é¡·ï¼ˆå…¶ä¸­æ°´é¢39å…¬é¡·ï¼‰ï¼Œä¸»è¦ç”±ç¼åå²›ã€ä¸œå²¸å’ŒåŒ—å²¸æ™¯åŒºç»„æˆã€‚ç¼åå²›ä¸Šæ ‘æœ¨è‹éƒï¼Œäº­å°æ¥¼é˜å¹½é™ï¼Œç™½å¡”è€¸ç«‹å±±å·…ï¼Œæˆä¸ºå…¬å›­çš„æ ‡å¿—ã€‚']\\n        \\n        Hypothesis: åŒ—æµ·å…¬å›­ä½äºåŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·ã€‚\\n        \\n        Please answer with the template below for all statement sentences:\\n\\n        Criteria: <Statement Sentence>, \\n        Supporting Evidence: <Identify and describe the location in the source where the information matches the statement. Provide a detailed, human-readable summary indicating the path or key details. if nothing matches, say NOTHING FOUND>\\n        Score: <Output a number between 0-10 where 0 is no information overlap and 10 is all information is overlapping>\\n        \\n\\n\", 'options': {'temperature': 0.0}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(temperature=0.0, model='ollama/qwen2', messages=[{'role': 'system', 'content': 'You are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.'}, {'role': 'user', 'content': 'QUESTION: åŒ—æµ·å…¬å›­åœ¨å“ªé‡Œï¼Ÿ\\n\\n        CONTEXT: Beihai Park\\n- åœ°ç†ä½ç½®\\n- åŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·\\n- æ°”å€™æ¡ä»¶\\n- æ¸©å¸¦å­£é£æ°”å€™\\n- å¼€æ”¾æ—¶é—´\\n- 4æœˆ1æ—¥-10æœˆ31æ—¥ 06:00-21:00ï¼›11æœˆ1æ—¥-æ¬¡å¹´3æœˆ31æ—¥ 06:30-20:00 [1]\\n-\\n        \\n        \\nPlease answer using the entire template below.\\n\\nTEMPLATE: \\nScore: <The score 0-10 based on the given criteria>\\nCriteria: <Provide the criteria for this evaluation>\\nSupporting Evidence: <Provide your reasons for scoring based on the listed criteria step by step. Tie it back to the evaluation being completed.>\\n '}])\u001b[0m\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0.0}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'qwen2', 'prompt': '### System:\\nYou are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.\\n\\n### User:\\nQUESTION: åŒ—æµ·å…¬å›­åœ¨å“ªé‡Œï¼Ÿ\\n\\n        CONTEXT: Beihai Park\\n- åœ°ç†ä½ç½®\\n- åŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·\\n- æ°”å€™æ¡ä»¶\\n- æ¸©å¸¦å­£é£æ°”å€™\\n- å¼€æ”¾æ—¶é—´\\n- 4æœˆ1æ—¥-10æœˆ31æ—¥ 06:00-21:00ï¼›11æœˆ1æ—¥-æ¬¡å¹´3æœˆ31æ—¥ 06:30-20:00 [1]\\n-\\n        \\n        \\nPlease answer using the entire template below.\\n\\nTEMPLATE: \\nScore: <The score 0-10 based on the given criteria>\\nCriteria: <Provide the criteria for this evaluation>\\nSupporting Evidence: <Provide your reasons for scoring based on the listed criteria step by step. Tie it back to the evaluation being completed.>\\n \\n\\n', 'options': {'temperature': 0.0}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(temperature=0.0, model='ollama/qwen2', messages=[{'role': 'system', 'content': \"You are a RELEVANCE grader; providing the relevance of the given RESPONSE to the given PROMPT.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long RESPONSES should score equally well as short RESPONSES.\\n\\n        - Answers that intentionally do not answer the question, such as 'I don't know' and model refusals, should also be counted as the most RELEVANT.\\n\\n        - RESPONSE must be relevant to the entire PROMPT to get a score of 10.\\n\\n        - RELEVANCE score should increase as the RESPONSE provides RELEVANT context to more parts of the PROMPT.\\n\\n        - RESPONSE that is RELEVANT to none of the PROMPT should get a score of 0.\\n\\n        - RESPONSE that is RELEVANT to some of the PROMPT should get as score of 2, 3, or 4. Higher score indicates more RELEVANCE.\\n\\n        - RESPONSE that is RELEVANT to most of the PROMPT should get a score between a 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - RESPONSE that is RELEVANT to the entire PROMPT should get a score of 9 or 10.\\n\\n        - RESPONSE that is RELEVANT and answers the entire PROMPT completely should get a score of 10.\\n\\n        - RESPONSE that confidently FALSE should get a score of 0.\\n\\n        - RESPONSE that is only seemingly RELEVANT should get a score of 0.\\n\\n        - Never elaborate.\\n        \"}, {'role': 'user', 'content': 'PROMPT: åŒ—æµ·å…¬å›­åœ¨å“ªé‡Œï¼Ÿ\\n\\n        RESPONSE: åŒ—æµ·å…¬å›­ä½äºåŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·ã€‚\\n\\n        \\nPlease answer using the entire template below.\\n\\nTEMPLATE: \\nScore: <The score 0-10 based on the given criteria>\\nCriteria: <Provide the criteria for this evaluation>\\nSupporting Evidence: <Provide your reasons for scoring based on the listed criteria step by step. Tie it back to the evaluation being completed.>\\n '}])\u001b[0m\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0.0}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'qwen2', 'prompt': \"### System:\\nYou are a RELEVANCE grader; providing the relevance of the given RESPONSE to the given PROMPT.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long RESPONSES should score equally well as short RESPONSES.\\n\\n        - Answers that intentionally do not answer the question, such as 'I don't know' and model refusals, should also be counted as the most RELEVANT.\\n\\n        - RESPONSE must be relevant to the entire PROMPT to get a score of 10.\\n\\n        - RELEVANCE score should increase as the RESPONSE provides RELEVANT context to more parts of the PROMPT.\\n\\n        - RESPONSE that is RELEVANT to none of the PROMPT should get a score of 0.\\n\\n        - RESPONSE that is RELEVANT to some of the PROMPT should get as score of 2, 3, or 4. Higher score indicates more RELEVANCE.\\n\\n        - RESPONSE that is RELEVANT to most of the PROMPT should get a score between a 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - RESPONSE that is RELEVANT to the entire PROMPT should get a score of 9 or 10.\\n\\n        - RESPONSE that is RELEVANT and answers the entire PROMPT completely should get a score of 10.\\n\\n        - RESPONSE that confidently FALSE should get a score of 0.\\n\\n        - RESPONSE that is only seemingly RELEVANT should get a score of 0.\\n\\n        - Never elaborate.\\n        \\n\\n### User:\\nPROMPT: åŒ—æµ·å…¬å›­åœ¨å“ªé‡Œï¼Ÿ\\n\\n        RESPONSE: åŒ—æµ·å…¬å›­ä½äºåŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·ã€‚\\n\\n        \\nPlease answer using the entire template below.\\n\\nTEMPLATE: \\nScore: <The score 0-10 based on the given criteria>\\nCriteria: <Provide the criteria for this evaluation>\\nSupporting Evidence: <Provide your reasons for scoring based on the listed criteria step by step. Tie it back to the evaluation being completed.>\\n \\n\\n\", 'options': {'temperature': 0.0}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(temperature=0.0, model='ollama/qwen2', messages=[{'role': 'system', 'content': 'You are a INFORMATION OVERLAP classifier; providing the overlap of information between the source and statement.\\n        Respond only as a number from 0 to 10 where 0 is no information overlap and 10 is all information is overlapping.\\n        Never elaborate.'}, {'role': 'user', 'content': \"SOURCE: ['Beihai Park\\\\n- åœ°ç†ä½ç½®\\\\n- åŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·\\\\n- æ°”å€™æ¡ä»¶\\\\n- æ¸©å¸¦å­£é£æ°”å€™\\\\n- å¼€æ”¾æ—¶é—´\\\\n- 4æœˆ1æ—¥-10æœˆ31æ—¥ 06:00-21:00ï¼›11æœˆ1æ—¥-æ¬¡å¹´3æœˆ31æ—¥ 06:30-20:00 [1]\\\\n-', '[32]åŒ—æµ·å…¬å›­ä¹Ÿæ˜¯å›½å®¶AAAAçº§æ—…æ¸¸æ™¯åŒºã€‚ [33]\\\\n2023å¹´4æœˆï¼ŒåŒ—æµ·å…¬å›­å…¥é€‰ç¬¬äºŒæ‰¹å›½å®¶çº§æ–‡æ˜æ—…æ¸¸ç¤ºèŒƒå•ä½å…¬ç¤ºåå•ã€‚ [37]\\\\n- ä¸­æ–‡å\\\\n- åŒ—æµ·å…¬å›­\\\\n- å¤–æ–‡å\\\\n- Beihai Park\\\\n- åœ°ç†ä½ç½®\\\\n-', '[1]å…¨å›­ä»¥åŒ—æµ·ä¸ºä¸­å¿ƒï¼Œå…¨å›­å åœ°69å…¬é¡·ï¼ˆå…¶ä¸­æ°´é¢39å…¬é¡·ï¼‰ï¼Œä¸»è¦ç”±ç¼åå²›ã€ä¸œå²¸å’ŒåŒ—å²¸æ™¯åŒºç»„æˆã€‚ç¼åå²›ä¸Šæ ‘æœ¨è‹éƒï¼Œäº­å°æ¥¼é˜å¹½é™ï¼Œç™½å¡”è€¸ç«‹å±±å·…ï¼Œæˆä¸ºå…¬å›­çš„æ ‡å¿—ã€‚']\\n        \\n        Hypothesis: åŒ—æµ·å…¬å›­ä½äºåŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·ã€‚\\n        \\n        Please answer with the template below for all statement sentences:\\n\\n        Criteria: <Statement Sentence>, \\n        Supporting Evidence: <Identify and describe the location in the source where the information matches the statement. Provide a detailed, human-readable summary indicating the path or key details. if nothing matches, say NOTHING FOUND>\\n        Score: <Output a number between 0-10 where 0 is no information overlap and 10 is all information is overlapping>\\n        \"}])\u001b[0m\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0.0}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'qwen2', 'prompt': \"### System:\\nYou are a INFORMATION OVERLAP classifier; providing the overlap of information between the source and statement.\\n        Respond only as a number from 0 to 10 where 0 is no information overlap and 10 is all information is overlapping.\\n        Never elaborate.\\n\\n### User:\\nSOURCE: ['Beihai Park\\\\n- åœ°ç†ä½ç½®\\\\n- åŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·\\\\n- æ°”å€™æ¡ä»¶\\\\n- æ¸©å¸¦å­£é£æ°”å€™\\\\n- å¼€æ”¾æ—¶é—´\\\\n- 4æœˆ1æ—¥-10æœˆ31æ—¥ 06:00-21:00ï¼›11æœˆ1æ—¥-æ¬¡å¹´3æœˆ31æ—¥ 06:30-20:00 [1]\\\\n-', '[32]åŒ—æµ·å…¬å›­ä¹Ÿæ˜¯å›½å®¶AAAAçº§æ—…æ¸¸æ™¯åŒºã€‚ [33]\\\\n2023å¹´4æœˆï¼ŒåŒ—æµ·å…¬å›­å…¥é€‰ç¬¬äºŒæ‰¹å›½å®¶çº§æ–‡æ˜æ—…æ¸¸ç¤ºèŒƒå•ä½å…¬ç¤ºåå•ã€‚ [37]\\\\n- ä¸­æ–‡å\\\\n- åŒ—æµ·å…¬å›­\\\\n- å¤–æ–‡å\\\\n- Beihai Park\\\\n- åœ°ç†ä½ç½®\\\\n-', '[1]å…¨å›­ä»¥åŒ—æµ·ä¸ºä¸­å¿ƒï¼Œå…¨å›­å åœ°69å…¬é¡·ï¼ˆå…¶ä¸­æ°´é¢39å…¬é¡·ï¼‰ï¼Œä¸»è¦ç”±ç¼åå²›ã€ä¸œå²¸å’ŒåŒ—å²¸æ™¯åŒºç»„æˆã€‚ç¼åå²›ä¸Šæ ‘æœ¨è‹éƒï¼Œäº­å°æ¥¼é˜å¹½é™ï¼Œç™½å¡”è€¸ç«‹å±±å·…ï¼Œæˆä¸ºå…¬å›­çš„æ ‡å¿—ã€‚']\\n        \\n        Hypothesis: åŒ—æµ·å…¬å›­ä½äºåŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·ã€‚\\n        \\n        Please answer with the template below for all statement sentences:\\n\\n        Criteria: <Statement Sentence>, \\n        Supporting Evidence: <Identify and describe the location in the source where the information matches the statement. Provide a detailed, human-readable summary indicating the path or key details. if nothing matches, say NOTHING FOUND>\\n        Score: <Output a number between 0-10 where 0 is no information overlap and 10 is all information is overlapping>\\n        \\n\\n\", 'options': {'temperature': 0.0}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(temperature=0.0, model='ollama/qwen2', messages=[{'role': 'system', 'content': \"You are a RELEVANCE grader; providing the relevance of the given RESPONSE to the given PROMPT.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long RESPONSES should score equally well as short RESPONSES.\\n\\n        - Answers that intentionally do not answer the question, such as 'I don't know' and model refusals, should also be counted as the most RELEVANT.\\n\\n        - RESPONSE must be relevant to the entire PROMPT to get a score of 10.\\n\\n        - RELEVANCE score should increase as the RESPONSE provides RELEVANT context to more parts of the PROMPT.\\n\\n        - RESPONSE that is RELEVANT to none of the PROMPT should get a score of 0.\\n\\n        - RESPONSE that is RELEVANT to some of the PROMPT should get as score of 2, 3, or 4. Higher score indicates more RELEVANCE.\\n\\n        - RESPONSE that is RELEVANT to most of the PROMPT should get a score between a 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - RESPONSE that is RELEVANT to the entire PROMPT should get a score of 9 or 10.\\n\\n        - RESPONSE that is RELEVANT and answers the entire PROMPT completely should get a score of 10.\\n\\n        - RESPONSE that confidently FALSE should get a score of 0.\\n\\n        - RESPONSE that is only seemingly RELEVANT should get a score of 0.\\n\\n        - Never elaborate.\\n        \"}, {'role': 'user', 'content': 'PROMPT: åŒ—æµ·å…¬å›­åœ¨å“ªé‡Œï¼Ÿ\\n\\n        RESPONSE: åŒ—æµ·å…¬å›­ä½äºåŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·ã€‚\\n\\n        \\nPlease answer using the entire template below.\\n\\nTEMPLATE: \\nScore: <The score 0-10 based on the given criteria>\\nCriteria: <Provide the criteria for this evaluation>\\nSupporting Evidence: <Provide your reasons for scoring based on the listed criteria step by step. Tie it back to the evaluation being completed.>\\n '}])\u001b[0m\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0.0}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'qwen2', 'prompt': \"### System:\\nYou are a RELEVANCE grader; providing the relevance of the given RESPONSE to the given PROMPT.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long RESPONSES should score equally well as short RESPONSES.\\n\\n        - Answers that intentionally do not answer the question, such as 'I don't know' and model refusals, should also be counted as the most RELEVANT.\\n\\n        - RESPONSE must be relevant to the entire PROMPT to get a score of 10.\\n\\n        - RELEVANCE score should increase as the RESPONSE provides RELEVANT context to more parts of the PROMPT.\\n\\n        - RESPONSE that is RELEVANT to none of the PROMPT should get a score of 0.\\n\\n        - RESPONSE that is RELEVANT to some of the PROMPT should get as score of 2, 3, or 4. Higher score indicates more RELEVANCE.\\n\\n        - RESPONSE that is RELEVANT to most of the PROMPT should get a score between a 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - RESPONSE that is RELEVANT to the entire PROMPT should get a score of 9 or 10.\\n\\n        - RESPONSE that is RELEVANT and answers the entire PROMPT completely should get a score of 10.\\n\\n        - RESPONSE that confidently FALSE should get a score of 0.\\n\\n        - RESPONSE that is only seemingly RELEVANT should get a score of 0.\\n\\n        - Never elaborate.\\n        \\n\\n### User:\\nPROMPT: åŒ—æµ·å…¬å›­åœ¨å“ªé‡Œï¼Ÿ\\n\\n        RESPONSE: åŒ—æµ·å…¬å›­ä½äºåŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·ã€‚\\n\\n        \\nPlease answer using the entire template below.\\n\\nTEMPLATE: \\nScore: <The score 0-10 based on the given criteria>\\nCriteria: <Provide the criteria for this evaluation>\\nSupporting Evidence: <Provide your reasons for scoring based on the listed criteria step by step. Tie it back to the evaluation being completed.>\\n \\n\\n\", 'options': {'temperature': 0.0}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(temperature=0.0, model='ollama/qwen2', messages=[{'role': 'system', 'content': 'You are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.'}, {'role': 'user', 'content': 'QUESTION: åŒ—æµ·å…¬å›­åœ¨å“ªé‡Œï¼Ÿ\\n\\n        CONTEXT: Beihai Park\\n- åœ°ç†ä½ç½®\\n- åŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·\\n- æ°”å€™æ¡ä»¶\\n- æ¸©å¸¦å­£é£æ°”å€™\\n- å¼€æ”¾æ—¶é—´\\n- 4æœˆ1æ—¥-10æœˆ31æ—¥ 06:00-21:00ï¼›11æœˆ1æ—¥-æ¬¡å¹´3æœˆ31æ—¥ 06:30-20:00 [1]\\n-\\n        \\n        \\nPlease answer using the entire template below.\\n\\nTEMPLATE: \\nScore: <The score 0-10 based on the given criteria>\\nCriteria: <Provide the criteria for this evaluation>\\nSupporting Evidence: <Provide your reasons for scoring based on the listed criteria step by step. Tie it back to the evaluation being completed.>\\n '}])\u001b[0m\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0.0}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'qwen2', 'prompt': '### System:\\nYou are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.\\n\\n### User:\\nQUESTION: åŒ—æµ·å…¬å›­åœ¨å“ªé‡Œï¼Ÿ\\n\\n        CONTEXT: Beihai Park\\n- åœ°ç†ä½ç½®\\n- åŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·\\n- æ°”å€™æ¡ä»¶\\n- æ¸©å¸¦å­£é£æ°”å€™\\n- å¼€æ”¾æ—¶é—´\\n- 4æœˆ1æ—¥-10æœˆ31æ—¥ 06:00-21:00ï¼›11æœˆ1æ—¥-æ¬¡å¹´3æœˆ31æ—¥ 06:30-20:00 [1]\\n-\\n        \\n        \\nPlease answer using the entire template below.\\n\\nTEMPLATE: \\nScore: <The score 0-10 based on the given criteria>\\nCriteria: <Provide the criteria for this evaluation>\\nSupporting Evidence: <Provide your reasons for scoring based on the listed criteria step by step. Tie it back to the evaluation being completed.>\\n \\n\\n', 'options': {'temperature': 0.0}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(temperature=0.0, model='ollama/qwen2', messages=[{'role': 'system', 'content': 'You are a INFORMATION OVERLAP classifier; providing the overlap of information between the source and statement.\\n        Respond only as a number from 0 to 10 where 0 is no information overlap and 10 is all information is overlapping.\\n        Never elaborate.'}, {'role': 'user', 'content': \"SOURCE: ['Beihai Park\\\\n- åœ°ç†ä½ç½®\\\\n- åŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·\\\\n- æ°”å€™æ¡ä»¶\\\\n- æ¸©å¸¦å­£é£æ°”å€™\\\\n- å¼€æ”¾æ—¶é—´\\\\n- 4æœˆ1æ—¥-10æœˆ31æ—¥ 06:00-21:00ï¼›11æœˆ1æ—¥-æ¬¡å¹´3æœˆ31æ—¥ 06:30-20:00 [1]\\\\n-', '[32]åŒ—æµ·å…¬å›­ä¹Ÿæ˜¯å›½å®¶AAAAçº§æ—…æ¸¸æ™¯åŒºã€‚ [33]\\\\n2023å¹´4æœˆï¼ŒåŒ—æµ·å…¬å›­å…¥é€‰ç¬¬äºŒæ‰¹å›½å®¶çº§æ–‡æ˜æ—…æ¸¸ç¤ºèŒƒå•ä½å…¬ç¤ºåå•ã€‚ [37]\\\\n- ä¸­æ–‡å\\\\n- åŒ—æµ·å…¬å›­\\\\n- å¤–æ–‡å\\\\n- Beihai Park\\\\n- åœ°ç†ä½ç½®\\\\n-', '[1]å…¨å›­ä»¥åŒ—æµ·ä¸ºä¸­å¿ƒï¼Œå…¨å›­å åœ°69å…¬é¡·ï¼ˆå…¶ä¸­æ°´é¢39å…¬é¡·ï¼‰ï¼Œä¸»è¦ç”±ç¼åå²›ã€ä¸œå²¸å’ŒåŒ—å²¸æ™¯åŒºç»„æˆã€‚ç¼åå²›ä¸Šæ ‘æœ¨è‹éƒï¼Œäº­å°æ¥¼é˜å¹½é™ï¼Œç™½å¡”è€¸ç«‹å±±å·…ï¼Œæˆä¸ºå…¬å›­çš„æ ‡å¿—ã€‚']\\n        \\n        Hypothesis: åŒ—æµ·å…¬å›­ä½äºåŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·ã€‚\\n        \\n        Please answer with the template below for all statement sentences:\\n\\n        Criteria: <Statement Sentence>, \\n        Supporting Evidence: <Identify and describe the location in the source where the information matches the statement. Provide a detailed, human-readable summary indicating the path or key details. if nothing matches, say NOTHING FOUND>\\n        Score: <Output a number between 0-10 where 0 is no information overlap and 10 is all information is overlapping>\\n        \"}])\u001b[0m\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0.0}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'qwen2', 'prompt': \"### System:\\nYou are a INFORMATION OVERLAP classifier; providing the overlap of information between the source and statement.\\n        Respond only as a number from 0 to 10 where 0 is no information overlap and 10 is all information is overlapping.\\n        Never elaborate.\\n\\n### User:\\nSOURCE: ['Beihai Park\\\\n- åœ°ç†ä½ç½®\\\\n- åŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·\\\\n- æ°”å€™æ¡ä»¶\\\\n- æ¸©å¸¦å­£é£æ°”å€™\\\\n- å¼€æ”¾æ—¶é—´\\\\n- 4æœˆ1æ—¥-10æœˆ31æ—¥ 06:00-21:00ï¼›11æœˆ1æ—¥-æ¬¡å¹´3æœˆ31æ—¥ 06:30-20:00 [1]\\\\n-', '[32]åŒ—æµ·å…¬å›­ä¹Ÿæ˜¯å›½å®¶AAAAçº§æ—…æ¸¸æ™¯åŒºã€‚ [33]\\\\n2023å¹´4æœˆï¼ŒåŒ—æµ·å…¬å›­å…¥é€‰ç¬¬äºŒæ‰¹å›½å®¶çº§æ–‡æ˜æ—…æ¸¸ç¤ºèŒƒå•ä½å…¬ç¤ºåå•ã€‚ [37]\\\\n- ä¸­æ–‡å\\\\n- åŒ—æµ·å…¬å›­\\\\n- å¤–æ–‡å\\\\n- Beihai Park\\\\n- åœ°ç†ä½ç½®\\\\n-', '[1]å…¨å›­ä»¥åŒ—æµ·ä¸ºä¸­å¿ƒï¼Œå…¨å›­å åœ°69å…¬é¡·ï¼ˆå…¶ä¸­æ°´é¢39å…¬é¡·ï¼‰ï¼Œä¸»è¦ç”±ç¼åå²›ã€ä¸œå²¸å’ŒåŒ—å²¸æ™¯åŒºç»„æˆã€‚ç¼åå²›ä¸Šæ ‘æœ¨è‹éƒï¼Œäº­å°æ¥¼é˜å¹½é™ï¼Œç™½å¡”è€¸ç«‹å±±å·…ï¼Œæˆä¸ºå…¬å›­çš„æ ‡å¿—ã€‚']\\n        \\n        Hypothesis: åŒ—æµ·å…¬å›­ä½äºåŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·ã€‚\\n        \\n        Please answer with the template below for all statement sentences:\\n\\n        Criteria: <Statement Sentence>, \\n        Supporting Evidence: <Identify and describe the location in the source where the information matches the statement. Provide a detailed, human-readable summary indicating the path or key details. if nothing matches, say NOTHING FOUND>\\n        Score: <Output a number between 0-10 where 0 is no information overlap and 10 is all information is overlapping>\\n        \\n\\n\", 'options': {'temperature': 0.0}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(temperature=0.0, model='ollama/qwen2', messages=[{'role': 'system', 'content': \"You are a RELEVANCE grader; providing the relevance of the given RESPONSE to the given PROMPT.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long RESPONSES should score equally well as short RESPONSES.\\n\\n        - Answers that intentionally do not answer the question, such as 'I don't know' and model refusals, should also be counted as the most RELEVANT.\\n\\n        - RESPONSE must be relevant to the entire PROMPT to get a score of 10.\\n\\n        - RELEVANCE score should increase as the RESPONSE provides RELEVANT context to more parts of the PROMPT.\\n\\n        - RESPONSE that is RELEVANT to none of the PROMPT should get a score of 0.\\n\\n        - RESPONSE that is RELEVANT to some of the PROMPT should get as score of 2, 3, or 4. Higher score indicates more RELEVANCE.\\n\\n        - RESPONSE that is RELEVANT to most of the PROMPT should get a score between a 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - RESPONSE that is RELEVANT to the entire PROMPT should get a score of 9 or 10.\\n\\n        - RESPONSE that is RELEVANT and answers the entire PROMPT completely should get a score of 10.\\n\\n        - RESPONSE that confidently FALSE should get a score of 0.\\n\\n        - RESPONSE that is only seemingly RELEVANT should get a score of 0.\\n\\n        - Never elaborate.\\n        \"}, {'role': 'user', 'content': 'PROMPT: åŒ—æµ·å…¬å›­åœ¨å“ªé‡Œï¼Ÿ\\n\\n        RESPONSE: åŒ—æµ·å…¬å›­ä½äºåŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·ã€‚\\n\\n        \\nPlease answer using the entire template below.\\n\\nTEMPLATE: \\nScore: <The score 0-10 based on the given criteria>\\nCriteria: <Provide the criteria for this evaluation>\\nSupporting Evidence: <Provide your reasons for scoring based on the listed criteria step by step. Tie it back to the evaluation being completed.>\\n '}])\u001b[0m\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0.0}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'qwen2', 'prompt': \"### System:\\nYou are a RELEVANCE grader; providing the relevance of the given RESPONSE to the given PROMPT.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long RESPONSES should score equally well as short RESPONSES.\\n\\n        - Answers that intentionally do not answer the question, such as 'I don't know' and model refusals, should also be counted as the most RELEVANT.\\n\\n        - RESPONSE must be relevant to the entire PROMPT to get a score of 10.\\n\\n        - RELEVANCE score should increase as the RESPONSE provides RELEVANT context to more parts of the PROMPT.\\n\\n        - RESPONSE that is RELEVANT to none of the PROMPT should get a score of 0.\\n\\n        - RESPONSE that is RELEVANT to some of the PROMPT should get as score of 2, 3, or 4. Higher score indicates more RELEVANCE.\\n\\n        - RESPONSE that is RELEVANT to most of the PROMPT should get a score between a 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - RESPONSE that is RELEVANT to the entire PROMPT should get a score of 9 or 10.\\n\\n        - RESPONSE that is RELEVANT and answers the entire PROMPT completely should get a score of 10.\\n\\n        - RESPONSE that confidently FALSE should get a score of 0.\\n\\n        - RESPONSE that is only seemingly RELEVANT should get a score of 0.\\n\\n        - Never elaborate.\\n        \\n\\n### User:\\nPROMPT: åŒ—æµ·å…¬å›­åœ¨å“ªé‡Œï¼Ÿ\\n\\n        RESPONSE: åŒ—æµ·å…¬å›­ä½äºåŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·ã€‚\\n\\n        \\nPlease answer using the entire template below.\\n\\nTEMPLATE: \\nScore: <The score 0-10 based on the given criteria>\\nCriteria: <Provide the criteria for this evaluation>\\nSupporting Evidence: <Provide your reasons for scoring based on the listed criteria step by step. Tie it back to the evaluation being completed.>\\n \\n\\n\", 'options': {'temperature': 0.0}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(temperature=0.0, model='ollama/qwen2', messages=[{'role': 'system', 'content': 'You are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.'}, {'role': 'user', 'content': 'QUESTION: åŒ—æµ·å…¬å›­åœ¨å“ªé‡Œï¼Ÿ\\n\\n        CONTEXT: Beihai Park\\n- åœ°ç†ä½ç½®\\n- åŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·\\n- æ°”å€™æ¡ä»¶\\n- æ¸©å¸¦å­£é£æ°”å€™\\n- å¼€æ”¾æ—¶é—´\\n- 4æœˆ1æ—¥-10æœˆ31æ—¥ 06:00-21:00ï¼›11æœˆ1æ—¥-æ¬¡å¹´3æœˆ31æ—¥ 06:30-20:00 [1]\\n-\\n        \\n        \\nPlease answer using the entire template below.\\n\\nTEMPLATE: \\nScore: <The score 0-10 based on the given criteria>\\nCriteria: <Provide the criteria for this evaluation>\\nSupporting Evidence: <Provide your reasons for scoring based on the listed criteria step by step. Tie it back to the evaluation being completed.>\\n '}])\u001b[0m\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0.0}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'qwen2', 'prompt': '### System:\\nYou are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.\\n\\n### User:\\nQUESTION: åŒ—æµ·å…¬å›­åœ¨å“ªé‡Œï¼Ÿ\\n\\n        CONTEXT: Beihai Park\\n- åœ°ç†ä½ç½®\\n- åŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·\\n- æ°”å€™æ¡ä»¶\\n- æ¸©å¸¦å­£é£æ°”å€™\\n- å¼€æ”¾æ—¶é—´\\n- 4æœˆ1æ—¥-10æœˆ31æ—¥ 06:00-21:00ï¼›11æœˆ1æ—¥-æ¬¡å¹´3æœˆ31æ—¥ 06:30-20:00 [1]\\n-\\n        \\n        \\nPlease answer using the entire template below.\\n\\nTEMPLATE: \\nScore: <The score 0-10 based on the given criteria>\\nCriteria: <Provide the criteria for this evaluation>\\nSupporting Evidence: <Provide your reasons for scoring based on the listed criteria step by step. Tie it back to the evaluation being completed.>\\n \\n\\n', 'options': {'temperature': 0.0}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(temperature=0.0, model='ollama/qwen2', messages=[{'role': 'system', 'content': 'You are a INFORMATION OVERLAP classifier; providing the overlap of information between the source and statement.\\n        Respond only as a number from 0 to 10 where 0 is no information overlap and 10 is all information is overlapping.\\n        Never elaborate.'}, {'role': 'user', 'content': \"SOURCE: ['Beihai Park\\\\n- åœ°ç†ä½ç½®\\\\n- åŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·\\\\n- æ°”å€™æ¡ä»¶\\\\n- æ¸©å¸¦å­£é£æ°”å€™\\\\n- å¼€æ”¾æ—¶é—´\\\\n- 4æœˆ1æ—¥-10æœˆ31æ—¥ 06:00-21:00ï¼›11æœˆ1æ—¥-æ¬¡å¹´3æœˆ31æ—¥ 06:30-20:00 [1]\\\\n-', '[32]åŒ—æµ·å…¬å›­ä¹Ÿæ˜¯å›½å®¶AAAAçº§æ—…æ¸¸æ™¯åŒºã€‚ [33]\\\\n2023å¹´4æœˆï¼ŒåŒ—æµ·å…¬å›­å…¥é€‰ç¬¬äºŒæ‰¹å›½å®¶çº§æ–‡æ˜æ—…æ¸¸ç¤ºèŒƒå•ä½å…¬ç¤ºåå•ã€‚ [37]\\\\n- ä¸­æ–‡å\\\\n- åŒ—æµ·å…¬å›­\\\\n- å¤–æ–‡å\\\\n- Beihai Park\\\\n- åœ°ç†ä½ç½®\\\\n-', '[1]å…¨å›­ä»¥åŒ—æµ·ä¸ºä¸­å¿ƒï¼Œå…¨å›­å åœ°69å…¬é¡·ï¼ˆå…¶ä¸­æ°´é¢39å…¬é¡·ï¼‰ï¼Œä¸»è¦ç”±ç¼åå²›ã€ä¸œå²¸å’ŒåŒ—å²¸æ™¯åŒºç»„æˆã€‚ç¼åå²›ä¸Šæ ‘æœ¨è‹éƒï¼Œäº­å°æ¥¼é˜å¹½é™ï¼Œç™½å¡”è€¸ç«‹å±±å·…ï¼Œæˆä¸ºå…¬å›­çš„æ ‡å¿—ã€‚']\\n        \\n        Hypothesis: åŒ—æµ·å…¬å›­ä½äºåŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·ã€‚\\n        \\n        Please answer with the template below for all statement sentences:\\n\\n        Criteria: <Statement Sentence>, \\n        Supporting Evidence: <Identify and describe the location in the source where the information matches the statement. Provide a detailed, human-readable summary indicating the path or key details. if nothing matches, say NOTHING FOUND>\\n        Score: <Output a number between 0-10 where 0 is no information overlap and 10 is all information is overlapping>\\n        \"}])\u001b[0m\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0.0}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'qwen2', 'prompt': \"### System:\\nYou are a INFORMATION OVERLAP classifier; providing the overlap of information between the source and statement.\\n        Respond only as a number from 0 to 10 where 0 is no information overlap and 10 is all information is overlapping.\\n        Never elaborate.\\n\\n### User:\\nSOURCE: ['Beihai Park\\\\n- åœ°ç†ä½ç½®\\\\n- åŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·\\\\n- æ°”å€™æ¡ä»¶\\\\n- æ¸©å¸¦å­£é£æ°”å€™\\\\n- å¼€æ”¾æ—¶é—´\\\\n- 4æœˆ1æ—¥-10æœˆ31æ—¥ 06:00-21:00ï¼›11æœˆ1æ—¥-æ¬¡å¹´3æœˆ31æ—¥ 06:30-20:00 [1]\\\\n-', '[32]åŒ—æµ·å…¬å›­ä¹Ÿæ˜¯å›½å®¶AAAAçº§æ—…æ¸¸æ™¯åŒºã€‚ [33]\\\\n2023å¹´4æœˆï¼ŒåŒ—æµ·å…¬å›­å…¥é€‰ç¬¬äºŒæ‰¹å›½å®¶çº§æ–‡æ˜æ—…æ¸¸ç¤ºèŒƒå•ä½å…¬ç¤ºåå•ã€‚ [37]\\\\n- ä¸­æ–‡å\\\\n- åŒ—æµ·å…¬å›­\\\\n- å¤–æ–‡å\\\\n- Beihai Park\\\\n- åœ°ç†ä½ç½®\\\\n-', '[1]å…¨å›­ä»¥åŒ—æµ·ä¸ºä¸­å¿ƒï¼Œå…¨å›­å åœ°69å…¬é¡·ï¼ˆå…¶ä¸­æ°´é¢39å…¬é¡·ï¼‰ï¼Œä¸»è¦ç”±ç¼åå²›ã€ä¸œå²¸å’ŒåŒ—å²¸æ™¯åŒºç»„æˆã€‚ç¼åå²›ä¸Šæ ‘æœ¨è‹éƒï¼Œäº­å°æ¥¼é˜å¹½é™ï¼Œç™½å¡”è€¸ç«‹å±±å·…ï¼Œæˆä¸ºå…¬å›­çš„æ ‡å¿—ã€‚']\\n        \\n        Hypothesis: åŒ—æµ·å…¬å›­ä½äºåŒ—äº¬å¸‚è¥¿åŸåŒºæ–‡æ´¥è¡—1å·ã€‚\\n        \\n        Please answer with the template below for all statement sentences:\\n\\n        Criteria: <Statement Sentence>, \\n        Supporting Evidence: <Identify and describe the location in the source where the information matches the statement. Provide a detailed, human-readable summary indicating the path or key details. if nothing matches, say NOTHING FOUND>\\n        Score: <Output a number between 0-10 where 0 is no information overlap and 10 is all information is overlapping>\\n        \\n\\n\", 'options': {'temperature': 0.0}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "litellm.set_verbose = True\n",
    "\n",
    "# or as context manager\n",
    "with tru_query_engine_recorder as recording:\n",
    "    query_engine.query(\"åŒ—æµ·å…¬å›­åœ¨å“ªé‡Œï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa7ea228-28f4-4d59-a9bb-073d39d393b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 63.7 ms, sys: 8.01 ms, total: 71.7 ms\n",
      "Wall time: 14 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "last_record = recording.records[-1]\n",
    "\n",
    "from trulens_eval.utils.display import get_feedback_result\n",
    "get_feedback_result(last_record, \"Context Relevance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84c20ec2-97c3-482f-a5dc-580660e8f5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.74 ms, sys: 53 Âµs, total: 8.79 ms\n",
      "Wall time: 8.72 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from trulens_eval.guardrails.llama import WithFeedbackFilterNodes\n",
    "\n",
    "# note: feedback function used for guardrail must only return a score, not also reasons\n",
    "f_context_relevance_score = Feedback(provider.context_relevance)\n",
    "\n",
    "filtered_query_engine = WithFeedbackFilterNodes(query_engine, feedback=f_context_relevance_score, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b46366-5286-4c2e-8b66-82d5d7bf5de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import litellm\n",
    "litellm.set_verbose=True\n",
    "\n",
    "tru_recorder = TruLlama(filtered_query_engine,\n",
    "    app_id='LlamaIndex_App1_Filtered',\n",
    "    feedbacks=[f_answer_relevance, f_context_relevance, f_groundedness])\n",
    "\n",
    "with tru_recorder as recording:\n",
    "    llm_response = filtered_query_engine.query(\"åŒ—æµ·å…¬å›­åœ¨å“ªé‡Œï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee6b0f6-ed00-43c1-88a7-921a1d72dc24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
