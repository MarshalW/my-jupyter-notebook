{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b1e12e5-816f-4c69-9a40-5cfaf3e8dcb9",
   "metadata": {},
   "source": [
    "# 在 4060Ti 16GB 下使用 vllm 加载 glm3-6b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50abe5e9-3f9c-48dd-ac90-bf7be69f1905",
   "metadata": {},
   "source": [
    "可以正常使用，消耗显存 `14422MiB`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb12589b-4f3b-4779-bf88-9f5624a90bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.1 ms, sys: 10.5 ms, total: 21.5 ms\n",
      "Wall time: 4.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture\n",
    "\n",
    "!pip install vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd0a3703-537e-4a09-baca-a4f92cb5e09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-01 11:03:32 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='/models/chatglm3-6b', speculative_config=None, tokenizer='/models/chatglm3-6b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/models/chatglm3-6b, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "WARNING 08-01 11:03:32 tokenizer.py:129] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "INFO 08-01 11:03:33 model_runner.py:680] Starting to load model /models/chatglm3-6b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf69c28e11f4d02a3c1194cd3d5a5df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pt checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-01 11:05:22 model_runner.py:692] Loading model weights took 11.6579 GB\n",
      "INFO 08-01 11:05:25 gpu_executor.py:102] # GPU blocks: 2964, # CPU blocks: 9362\n",
      "INFO 08-01 11:05:27 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-01 11:05:27 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-01 11:05:53 model_runner.py:1181] Graph capturing finished in 27 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 4/4 [00:00<00:00,  5.18it/s, est. speed input: 38.88 toks/s, output: 66.09 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Hello, my name is', Generated text: ' [insert name] and I am a [insert job title]. I am ['\n",
      "Prompt: 'The president of the United States is', Generated text: ' the leader of the free world. The president has the power to declare war,'\n",
      "Prompt: 'The capital of France is', Generated text: ' Paris.'\n",
      "Prompt: 'The future of AI is', Generated text: ' bright, but also complex and uncertain. The potential for AI to transform the way'\n",
      "CPU times: user 33.6 s, sys: 9.33 s, total: 42.9 s\n",
      "Wall time: 2min 22s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "\n",
    "llm = LLM(model=\"/models/chatglm3-6b\", trust_remote_code=True)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f950766f-a5ab-4a24-8564-ff9a20957caa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
