{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b00d6109-e6c8-42d5-9b92-0057854e5800",
   "metadata": {},
   "source": [
    "# TruLens 集成 LlamaIndex 的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105385a9-7a76-424b-8ce5-f777d3d0bc2c",
   "metadata": {},
   "source": [
    "参考：\n",
    "\n",
    "- https://github.com/truera/trulens/blob/main/trulens_eval/examples/expositional/models/ollama_quickstart.ipynb\n",
    "- https://www.trulens.org/trulens_eval/getting_started/quickstarts/llama_index_quickstart/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c222636-1a5e-45c7-9502-07e27e16b30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.7 ms, sys: 4.99 ms, total: 16.7 ms\n",
      "Wall time: 2.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture\n",
    "\n",
    "!pip install trulens_eval llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3febacbf-dc3b-4b70-bde4-115e33204398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.3 ms, sys: 3.45 ms, total: 17.7 ms\n",
      "Wall time: 43.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import nltk\n",
    "nltk.set_proxy('http://myproxy:7890')\n",
    "\n",
    "from trulens_eval import Tru\n",
    "tru = Tru()\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd7ae214-3b50-492f-9f02-8aeb0dbbf370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 958 ms, sys: 80.6 ms, total: 1.04 s\n",
      "Wall time: 21.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "Settings.chunk_size = 128\n",
    "Settings.chunk_overlap = 16\n",
    "\n",
    "Settings.llm = OpenAILike(\n",
    "    model=\"qwen2\", \n",
    "    api_base=\"http://monkey:11434/v1\", \n",
    "    api_key=\"ollama\",\n",
    "    is_chat_model=True,\n",
    "    temperature=0.1,\n",
    "    request_timeout=60.0\n",
    ")\n",
    "\n",
    "Settings.embed_model =OllamaEmbedding(\n",
    "    model_name=\"quentinz/bge-large-zh-v1.5\",\n",
    "    base_url=\"http://monkey:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0}, # -mirostat N 使用 Mirostat 采样。\n",
    ")\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007da656-8ccc-497c-ba5c-116172f260a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not mention what the author did growing up; it discusses topics such as company growth, hiring practices, and decision-making in a startup environment. Therefore, based on this information alone, there is no way to determine what the author did growing up.\n",
      "CPU times: user 65.2 ms, sys: 4.5 ms, total: 69.7 ms\n",
      "Wall time: 2.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71eb89e0-1a73-43d9-94a0-07048a537d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.96 ms, sys: 11.8 ms, total: 18.8 ms\n",
      "Wall time: 1.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture\n",
    "\n",
    "!pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f97833df-6388-42b0-90d3-c6a9e2cf430e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.5 ms, sys: 7.92 ms, total: 59.4 ms\n",
      "Wall time: 59 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import litellm\n",
    "\n",
    "from trulens_eval.feedback.provider import LiteLLM\n",
    "\n",
    "litellm.set_verbose = False\n",
    "\n",
    "ollama_provider = LiteLLM(\n",
    "    model_engine=\"ollama/qwen2\", api_base=\"http://monkey:11434\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3354343-90a2-4bb3-b8dd-26e4f6557081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Groundedness, input source will be set to __record__.app.query.rets.source_nodes[:].node.text.collect() .\n",
      "✅ In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Context Relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Context Relevance, input context will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
      "CPU times: user 21.9 ms, sys: 7.77 ms, total: 29.7 ms\n",
      "Wall time: 29.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from trulens_eval.feedback.provider import OpenAI\n",
    "from trulens_eval import Feedback\n",
    "import numpy as np\n",
    "\n",
    "# Initialize provider class\n",
    "provider = ollama_provider\n",
    "\n",
    "# select context to be used in feedback. the location of context is app specific.\n",
    "from trulens_eval.app import App\n",
    "context = App.select_context(query_engine)\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(provider.groundedness_measure_with_cot_reasons, name = \"Groundedness\")\n",
    "    .on(context.collect()) # collect context chunks into a list\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_answer_relevance = (\n",
    "    Feedback(provider.relevance_with_cot_reasons, name = \"Answer Relevance\")\n",
    "    .on_input_output()\n",
    ")\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(provider.context_relevance_with_cot_reasons, name = \"Context Relevance\")\n",
    "    .on_input()\n",
    "    .on(context)\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "478d13d4-0b84-4060-93fa-fe369532f6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 294 ms, sys: 7.15 ms, total: 301 ms\n",
      "Wall time: 312 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from trulens_eval import TruLlama\n",
    "tru_query_engine_recorder = TruLlama(query_engine,\n",
    "    app_id='LlamaIndex_App1',\n",
    "    feedbacks=[f_groundedness, f_answer_relevance, f_context_relevance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "451b4bd4-7bf6-458c-a1f9-526e37db0a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.54 s, sys: 24.6 ms, total: 1.57 s\n",
      "Wall time: 2.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "litellm.set_verbose = True\n",
    "\n",
    "# or as context manager\n",
    "with tru_query_engine_recorder as recording:\n",
    "    query_engine.query(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4823a95a-b0a5-41fc-b5ea-79850917d765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.73 ms, sys: 126 µs, total: 5.86 ms\n",
      "Wall time: 5.22 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "last_record = recording.records[-1]\n",
    "\n",
    "from trulens_eval.utils.display import get_feedback_result\n",
    "get_feedback_result(last_record, \"Context Relevance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fc0c1cf-8150-4d3f-b8ec-d9a3b14ac604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.49 ms, sys: 108 µs, total: 9.6 ms\n",
      "Wall time: 9.27 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from trulens_eval.guardrails.llama import WithFeedbackFilterNodes\n",
    "\n",
    "# note: feedback function used for guardrail must only return a score, not also reasons\n",
    "f_context_relevance_score = Feedback(provider.context_relevance)\n",
    "\n",
    "filtered_query_engine = WithFeedbackFilterNodes(query_engine, feedback=f_context_relevance_score, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8be34922-c196-4b25-a0db-8f07eadab7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(temperature=0.0, model='ollama/qwen2', messages=[{'role': 'system', 'content': 'You are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.'}, {'role': 'user', 'content': \"QUESTION: What did the author do growing up?\\n\\n        CONTEXT: You're growing 7x a year. Just don't hire too many more people and you'll soon be profitable, and then you'll control your own destiny.\\n\\nAlas I hired lots more people, partly because our investors wanted me to, and partly because that's what startups did during the Internet Bubble. A company with just a handful of employees would have seemed amateurish.\\n        \\n        RELEVANCE: \"}])\u001b[0m\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0.0}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'qwen2', 'prompt': \"### System:\\nYou are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.\\n\\n### User:\\nQUESTION: What did the author do growing up?\\n\\n        CONTEXT: You're growing 7x a year. Just don't hire too many more people and you'll soon be profitable, and then you'll control your own destiny.\\n\\nAlas I hired lots more people, partly because our investors wanted me to, and partly because that's what startups did during the Internet Bubble. A company with just a handful of employees would have seemed amateurish.\\n        \\n        RELEVANCE: \\n\\n\", 'options': {'temperature': 0.0}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(temperature=0.0, model='ollama/qwen2', messages=[{'role': 'system', 'content': 'You are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.'}, {'role': 'user', 'content': \"QUESTION: What did the author do growing up?\\n\\n        CONTEXT: How should I choose what to do? Well, how had I chosen what to work on in the past? I wrote an essay for myself to answer that question, and I was surprised how long and messy the answer turned out to be. If this surprised me, who'd lived it, then I thought perhaps it would be interesting to other people, and encouraging to those with similarly messy lives.\\n        \\n        RELEVANCE: \"}])\u001b[0m\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0.0}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'qwen2', 'prompt': \"### System:\\nYou are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.\\n\\n### User:\\nQUESTION: What did the author do growing up?\\n\\n        CONTEXT: How should I choose what to do? Well, how had I chosen what to work on in the past? I wrote an essay for myself to answer that question, and I was surprised how long and messy the answer turned out to be. If this surprised me, who'd lived it, then I thought perhaps it would be interesting to other people, and encouraging to those with similarly messy lives.\\n        \\n        RELEVANCE: \\n\\n\", 'options': {'temperature': 0.0}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(temperature=0.0, model='ollama/qwen2', messages=[{'role': 'system', 'content': 'You are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.'}, {'role': 'user', 'content': \"QUESTION: What did the author do growing up?\\n\\n        CONTEXT: Another thing I didn't get at the time is that growth rate is the ultimate test of a startup. Our growth rate was fine. We had about 70 stores at the end of 1996 and about 500 at the end of 1997. I mistakenly thought the thing that mattered was the absolute number of users.\\n        \\n        RELEVANCE: \"}])\u001b[0m\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0.0}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'qwen2', 'prompt': \"### System:\\nYou are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.\\n\\n### User:\\nQUESTION: What did the author do growing up?\\n\\n        CONTEXT: Another thing I didn't get at the time is that growth rate is the ultimate test of a startup. Our growth rate was fine. We had about 70 stores at the end of 1996 and about 500 at the end of 1997. I mistakenly thought the thing that mattered was the absolute number of users.\\n        \\n        RELEVANCE: \\n\\n\", 'options': {'temperature': 0.0}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(temperature=0.0, model='ollama/qwen2', messages=[{'role': 'system', 'content': 'You are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.'}, {'role': 'user', 'content': \"QUESTION: What did the author do growing up?\\n\\n        CONTEXT: You're growing 7x a year. Just don't hire too many more people and you'll soon be profitable, and then you'll control your own destiny.\\n\\nAlas I hired lots more people, partly because our investors wanted me to, and partly because that's what startups did during the Internet Bubble. A company with just a handful of employees would have seemed amateurish.\\n        \\n        RELEVANCE: \"}])\u001b[0m\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0.0}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'qwen2', 'prompt': \"### System:\\nYou are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.\\n\\n### User:\\nQUESTION: What did the author do growing up?\\n\\n        CONTEXT: You're growing 7x a year. Just don't hire too many more people and you'll soon be profitable, and then you'll control your own destiny.\\n\\nAlas I hired lots more people, partly because our investors wanted me to, and partly because that's what startups did during the Internet Bubble. A company with just a handful of employees would have seemed amateurish.\\n        \\n        RELEVANCE: \\n\\n\", 'options': {'temperature': 0.0}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(temperature=0.0, model='ollama/qwen2', messages=[{'role': 'system', 'content': 'You are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.'}, {'role': 'user', 'content': \"QUESTION: What did the author do growing up?\\n\\n        CONTEXT: How should I choose what to do? Well, how had I chosen what to work on in the past? I wrote an essay for myself to answer that question, and I was surprised how long and messy the answer turned out to be. If this surprised me, who'd lived it, then I thought perhaps it would be interesting to other people, and encouraging to those with similarly messy lives.\\n        \\n        RELEVANCE: \"}])\u001b[0m\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0.0}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'qwen2', 'prompt': \"### System:\\nYou are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.\\n\\n### User:\\nQUESTION: What did the author do growing up?\\n\\n        CONTEXT: How should I choose what to do? Well, how had I chosen what to work on in the past? I wrote an essay for myself to answer that question, and I was surprised how long and messy the answer turned out to be. If this surprised me, who'd lived it, then I thought perhaps it would be interesting to other people, and encouraging to those with similarly messy lives.\\n        \\n        RELEVANCE: \\n\\n\", 'options': {'temperature': 0.0}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(temperature=0.0, model='ollama/qwen2', messages=[{'role': 'system', 'content': 'You are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.'}, {'role': 'user', 'content': \"QUESTION: What did the author do growing up?\\n\\n        CONTEXT: Another thing I didn't get at the time is that growth rate is the ultimate test of a startup. Our growth rate was fine. We had about 70 stores at the end of 1996 and about 500 at the end of 1997. I mistakenly thought the thing that mattered was the absolute number of users.\\n        \\n        RELEVANCE: \"}])\u001b[0m\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0.0}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'qwen2', 'prompt': \"### System:\\nYou are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.\\n\\n### User:\\nQUESTION: What did the author do growing up?\\n\\n        CONTEXT: Another thing I didn't get at the time is that growth rate is the ultimate test of a startup. Our growth rate was fine. We had about 70 stores at the end of 1996 and about 500 at the end of 1997. I mistakenly thought the thing that mattered was the absolute number of users.\\n        \\n        RELEVANCE: \\n\\n\", 'options': {'temperature': 0.0}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(temperature=0.0, model='ollama/qwen2', messages=[{'role': 'system', 'content': 'You are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.'}, {'role': 'user', 'content': \"QUESTION: What did the author do growing up?\\n\\n        CONTEXT: You're growing 7x a year. Just don't hire too many more people and you'll soon be profitable, and then you'll control your own destiny.\\n\\nAlas I hired lots more people, partly because our investors wanted me to, and partly because that's what startups did during the Internet Bubble. A company with just a handful of employees would have seemed amateurish.\\n        \\n        RELEVANCE: \"}])\u001b[0m\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0.0}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'qwen2', 'prompt': \"### System:\\nYou are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.\\n\\n### User:\\nQUESTION: What did the author do growing up?\\n\\n        CONTEXT: You're growing 7x a year. Just don't hire too many more people and you'll soon be profitable, and then you'll control your own destiny.\\n\\nAlas I hired lots more people, partly because our investors wanted me to, and partly because that's what startups did during the Internet Bubble. A company with just a handful of employees would have seemed amateurish.\\n        \\n        RELEVANCE: \\n\\n\", 'options': {'temperature': 0.0}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(temperature=0.0, model='ollama/qwen2', messages=[{'role': 'system', 'content': 'You are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.'}, {'role': 'user', 'content': \"QUESTION: What did the author do growing up?\\n\\n        CONTEXT: How should I choose what to do? Well, how had I chosen what to work on in the past? I wrote an essay for myself to answer that question, and I was surprised how long and messy the answer turned out to be. If this surprised me, who'd lived it, then I thought perhaps it would be interesting to other people, and encouraging to those with similarly messy lives.\\n        \\n        RELEVANCE: \"}])\u001b[0m\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0.0}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'qwen2', 'prompt': \"### System:\\nYou are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.\\n\\n### User:\\nQUESTION: What did the author do growing up?\\n\\n        CONTEXT: How should I choose what to do? Well, how had I chosen what to work on in the past? I wrote an essay for myself to answer that question, and I was surprised how long and messy the answer turned out to be. If this surprised me, who'd lived it, then I thought perhaps it would be interesting to other people, and encouraging to those with similarly messy lives.\\n        \\n        RELEVANCE: \\n\\n\", 'options': {'temperature': 0.0}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(temperature=0.0, model='ollama/qwen2', messages=[{'role': 'system', 'content': 'You are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.'}, {'role': 'user', 'content': \"QUESTION: What did the author do growing up?\\n\\n        CONTEXT: Another thing I didn't get at the time is that growth rate is the ultimate test of a startup. Our growth rate was fine. We had about 70 stores at the end of 1996 and about 500 at the end of 1997. I mistakenly thought the thing that mattered was the absolute number of users.\\n        \\n        RELEVANCE: \"}])\u001b[0m\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0.0}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'qwen2', 'prompt': \"### System:\\nYou are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.\\n\\n### User:\\nQUESTION: What did the author do growing up?\\n\\n        CONTEXT: Another thing I didn't get at the time is that growth rate is the ultimate test of a startup. Our growth rate was fine. We had about 70 stores at the end of 1996 and about 500 at the end of 1997. I mistakenly thought the thing that mattered was the absolute number of users.\\n        \\n        RELEVANCE: \\n\\n\", 'options': {'temperature': 0.0}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(temperature=0.0, model='ollama/qwen2', messages=[{'role': 'system', 'content': 'You are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.'}, {'role': 'user', 'content': \"QUESTION: What did the author do growing up?\\n\\n        CONTEXT: You're growing 7x a year. Just don't hire too many more people and you'll soon be profitable, and then you'll control your own destiny.\\n\\nAlas I hired lots more people, partly because our investors wanted me to, and partly because that's what startups did during the Internet Bubble. A company with just a handful of employees would have seemed amateurish.\\n        \\n        RELEVANCE: \"}])\u001b[0m\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0.0}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'qwen2', 'prompt': \"### System:\\nYou are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.\\n\\n### User:\\nQUESTION: What did the author do growing up?\\n\\n        CONTEXT: You're growing 7x a year. Just don't hire too many more people and you'll soon be profitable, and then you'll control your own destiny.\\n\\nAlas I hired lots more people, partly because our investors wanted me to, and partly because that's what startups did during the Internet Bubble. A company with just a handful of employees would have seemed amateurish.\\n        \\n        RELEVANCE: \\n\\n\", 'options': {'temperature': 0.0}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(temperature=0.0, model='ollama/qwen2', messages=[{'role': 'system', 'content': 'You are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.'}, {'role': 'user', 'content': \"QUESTION: What did the author do growing up?\\n\\n        CONTEXT: How should I choose what to do? Well, how had I chosen what to work on in the past? I wrote an essay for myself to answer that question, and I was surprised how long and messy the answer turned out to be. If this surprised me, who'd lived it, then I thought perhaps it would be interesting to other people, and encouraging to those with similarly messy lives.\\n        \\n        RELEVANCE: \"}])\u001b[0m\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0.0}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'qwen2', 'prompt': \"### System:\\nYou are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.\\n\\n### User:\\nQUESTION: What did the author do growing up?\\n\\n        CONTEXT: How should I choose what to do? Well, how had I chosen what to work on in the past? I wrote an essay for myself to answer that question, and I was surprised how long and messy the answer turned out to be. If this surprised me, who'd lived it, then I thought perhaps it would be interesting to other people, and encouraging to those with similarly messy lives.\\n        \\n        RELEVANCE: \\n\\n\", 'options': {'temperature': 0.0}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(temperature=0.0, model='ollama/qwen2', messages=[{'role': 'system', 'content': 'You are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.'}, {'role': 'user', 'content': \"QUESTION: What did the author do growing up?\\n\\n        CONTEXT: Another thing I didn't get at the time is that growth rate is the ultimate test of a startup. Our growth rate was fine. We had about 70 stores at the end of 1996 and about 500 at the end of 1997. I mistakenly thought the thing that mattered was the absolute number of users.\\n        \\n        RELEVANCE: \"}])\u001b[0m\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0.0}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'qwen2', 'prompt': \"### System:\\nYou are a RELEVANCE grader; providing the relevance of the given CONTEXT to the given QUESTION.\\n        Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\n        A few additional scoring guidelines:\\n\\n        - Long CONTEXTS should score equally well as short CONTEXTS.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides more RELEVANT context to the QUESTION.\\n\\n        - RELEVANCE score should increase as the CONTEXTS provides RELEVANT context to more parts of the QUESTION.\\n\\n        - CONTEXT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n        - CONTEXT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n        - Never elaborate.\\n\\n### User:\\nQUESTION: What did the author do growing up?\\n\\n        CONTEXT: Another thing I didn't get at the time is that growth rate is the ultimate test of a startup. Our growth rate was fine. We had about 70 stores at the end of 1996 and about 500 at the end of 1997. I mistakenly thought the thing that mattered was the absolute number of users.\\n        \\n        RELEVANCE: \\n\\n\", 'options': {'temperature': 0.0}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Endpoint litellm request failed 4 time(s): \n\tlitellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff174205db0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n\tlitellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff174224610>: Failed to establish a new connection: [Errno 111] Connection refused'))\n\tlitellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff174226ce0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n\tlitellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff17423d3f0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:5\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trulens_eval/app.py:1063\u001b[0m, in \u001b[0;36mApp.__exit__\u001b[0;34m(self, exc_type, exc_value, exc_tb)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecording_contexts\u001b[38;5;241m.\u001b[39mreset(ctx\u001b[38;5;241m.\u001b[39mtoken)\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1063\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m<timed exec>:6\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trulens_eval/instruments.py:601\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# TODO(piotrm): need to track costs of awaiting the ret in the\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;66;03m# below.\u001b[39;00m\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrap_awaitable(rets, on_done\u001b[38;5;241m=\u001b[39mhandle_done)\n\u001b[0;32m--> 601\u001b[0m \u001b[43mhandle_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rets\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trulens_eval/instruments.py:570\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper.<locals>.handle_done\u001b[0;34m(rets)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;66;03m# If stack has only 1 thing on it, we are looking at a \"root\u001b[39;00m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;66;03m# call\". Create a record of the result and notify the app:\u001b[39;00m\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(stack) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    568\u001b[0m         \u001b[38;5;66;03m# If this is a root call, notify app to add the completed record\u001b[39;00m\n\u001b[1;32m    569\u001b[0m         \u001b[38;5;66;03m# into its containers:\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m         records[ctx] \u001b[38;5;241m=\u001b[39m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_add_record\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m            \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m            \u001b[49m\u001b[43msig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbindings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbindings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m            \u001b[49m\u001b[43mret\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m            \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m            \u001b[49m\u001b[43mperf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmod_base_schema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPerf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_time\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexisting_record\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trulens_eval/app.py:1137\u001b[0m, in \u001b[0;36mApp.on_add_record\u001b[0;34m(self, ctx, func, sig, bindings, ret, error, perf, cost, existing_record)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;66;03m# May block on DB.\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_error(record\u001b[38;5;241m=\u001b[39mrecord, error\u001b[38;5;241m=\u001b[39merror)\n\u001b[0;32m-> 1137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# Will block on DB, but not on feedback evaluation, depending on\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# FeedbackMode:\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m record\u001b[38;5;241m.\u001b[39mfeedback_and_future_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_record(record\u001b[38;5;241m=\u001b[39mrecord)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trulens_eval/instruments.py:511\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;66;03m# Using sig bind here so we can produce a list of key-value\u001b[39;00m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;66;03m# pairs even if positional arguments were provided.\u001b[39;00m\n\u001b[1;32m    509\u001b[0m     bindings: BoundArguments \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 511\u001b[0m     rets, cost \u001b[38;5;241m=\u001b[39m \u001b[43mmod_endpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEndpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_all_costs_tally\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    516\u001b[0m     error \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trulens_eval/feedback/provider/endpoint/base.py:503\u001b[0m, in \u001b[0;36mEndpoint.track_all_costs_tally\u001b[0;34m(_Endpoint__func, with_openai, with_hugs, with_litellm, with_bedrock, with_cortex, *args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrack_all_costs_tally\u001b[39m(\n\u001b[1;32m    489\u001b[0m     __func: mod_asynchro_utils\u001b[38;5;241m.\u001b[39mCallableMaybeAwaitable[A, T],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    497\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[T, mod_base_schema\u001b[38;5;241m.\u001b[39mCost]:\n\u001b[1;32m    498\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;124;03m    Track costs of all of the apis we can currently track, over the\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;124;03m    execution of thunk.\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 503\u001b[0m     result, cbs \u001b[38;5;241m=\u001b[39m \u001b[43mEndpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_all_costs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43m__func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_openai\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_openai\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_hugs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_hugs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_litellm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_litellm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_bedrock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_bedrock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_cortex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_cortex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cbs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    515\u001b[0m         \u001b[38;5;66;03m# Otherwise sum returns \"0\" below.\u001b[39;00m\n\u001b[1;32m    516\u001b[0m         costs \u001b[38;5;241m=\u001b[39m mod_base_schema\u001b[38;5;241m.\u001b[39mCost()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trulens_eval/feedback/provider/endpoint/base.py:483\u001b[0m, in \u001b[0;36mEndpoint.track_all_costs\u001b[0;34m(_Endpoint__func, with_openai, with_hugs, with_litellm, with_bedrock, with_cortex, *args, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    475\u001b[0m             logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    476\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not initialize endpoint \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    477\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPossibly missing key(s). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    480\u001b[0m                 e,\n\u001b[1;32m    481\u001b[0m             )\n\u001b[0;32m--> 483\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEndpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_track_costs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43m__func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_endpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trulens_eval/feedback/provider/endpoint/base.py:581\u001b[0m, in \u001b[0;36mEndpoint._track_costs\u001b[0;34m(_Endpoint__func, with_endpoints, *args, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(callback)\n\u001b[1;32m    580\u001b[0m \u001b[38;5;66;03m# Call the function.\u001b[39;00m\n\u001b[0;32m--> 581\u001b[0m result: T \u001b[38;5;241m=\u001b[39m \u001b[43m__func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;66;03m# Return result and only the callbacks created here. Outer thunks might\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;66;03m# return others.\u001b[39;00m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result, callbacks\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trulens_eval/guardrails/llama.py:91\u001b[0m, in \u001b[0;36mWithFeedbackFilterNodes.query\u001b[0;34m(self, query, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m as_completed(future_to_node):\n\u001b[1;32m     90\u001b[0m     node \u001b[38;5;241m=\u001b[39m future_to_node[future]\n\u001b[0;32m---> 91\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mfloat\u001b[39m):\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     94\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGuardrails can only be used with feedback functions that return a float.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m         )\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trulens_eval/utils/python.py:475\u001b[0m, in \u001b[0;36m_future_target_wrapper\u001b[0;34m(stack, context, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var, value \u001b[38;5;129;01min\u001b[39;00m context\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    473\u001b[0m     var\u001b[38;5;241m.\u001b[39mset(value)\n\u001b[0;32m--> 475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trulens_eval/guardrails/llama.py:85\u001b[0m, in \u001b[0;36mWithFeedbackFilterNodes.query.<locals>.<lambda>\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     79\u001b[0m nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_engine\u001b[38;5;241m.\u001b[39mretrieve(query_bundle\u001b[38;5;241m=\u001b[39mquery)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(nodes))) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     82\u001b[0m     future_to_node \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     83\u001b[0m         ex\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m node\u001b[38;5;241m=\u001b[39mnode: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\n\u001b[0;32m---> 85\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeedback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m         ): node \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[1;32m     87\u001b[0m     }\n\u001b[1;32m     88\u001b[0m     filtered \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m as_completed(future_to_node):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trulens_eval/instruments.py:601\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# TODO(piotrm): need to track costs of awaiting the ret in the\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;66;03m# below.\u001b[39;00m\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrap_awaitable(rets, on_done\u001b[38;5;241m=\u001b[39mhandle_done)\n\u001b[0;32m--> 601\u001b[0m \u001b[43mhandle_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rets\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trulens_eval/instruments.py:585\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper.<locals>.handle_done\u001b[0;34m(rets)\u001b[0m\n\u001b[1;32m    570\u001b[0m         records[ctx] \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39mapp\u001b[38;5;241m.\u001b[39mon_add_record(\n\u001b[1;32m    571\u001b[0m             ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    572\u001b[0m             func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    581\u001b[0m             existing_record\u001b[38;5;241m=\u001b[39mrecords\u001b[38;5;241m.\u001b[39mget(ctx)\n\u001b[1;32m    582\u001b[0m         )\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m records\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trulens_eval/instruments.py:511\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;66;03m# Using sig bind here so we can produce a list of key-value\u001b[39;00m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;66;03m# pairs even if positional arguments were provided.\u001b[39;00m\n\u001b[1;32m    509\u001b[0m     bindings: BoundArguments \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 511\u001b[0m     rets, cost \u001b[38;5;241m=\u001b[39m \u001b[43mmod_endpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEndpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_all_costs_tally\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    516\u001b[0m     error \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trulens_eval/feedback/provider/endpoint/base.py:503\u001b[0m, in \u001b[0;36mEndpoint.track_all_costs_tally\u001b[0;34m(_Endpoint__func, with_openai, with_hugs, with_litellm, with_bedrock, with_cortex, *args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrack_all_costs_tally\u001b[39m(\n\u001b[1;32m    489\u001b[0m     __func: mod_asynchro_utils\u001b[38;5;241m.\u001b[39mCallableMaybeAwaitable[A, T],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    497\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[T, mod_base_schema\u001b[38;5;241m.\u001b[39mCost]:\n\u001b[1;32m    498\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;124;03m    Track costs of all of the apis we can currently track, over the\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;124;03m    execution of thunk.\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 503\u001b[0m     result, cbs \u001b[38;5;241m=\u001b[39m \u001b[43mEndpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_all_costs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43m__func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_openai\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_openai\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_hugs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_hugs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_litellm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_litellm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_bedrock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_bedrock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_cortex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_cortex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cbs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    515\u001b[0m         \u001b[38;5;66;03m# Otherwise sum returns \"0\" below.\u001b[39;00m\n\u001b[1;32m    516\u001b[0m         costs \u001b[38;5;241m=\u001b[39m mod_base_schema\u001b[38;5;241m.\u001b[39mCost()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trulens_eval/feedback/provider/endpoint/base.py:483\u001b[0m, in \u001b[0;36mEndpoint.track_all_costs\u001b[0;34m(_Endpoint__func, with_openai, with_hugs, with_litellm, with_bedrock, with_cortex, *args, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    475\u001b[0m             logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    476\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not initialize endpoint \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    477\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPossibly missing key(s). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    480\u001b[0m                 e,\n\u001b[1;32m    481\u001b[0m             )\n\u001b[0;32m--> 483\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEndpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_track_costs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43m__func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_endpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trulens_eval/feedback/provider/endpoint/base.py:581\u001b[0m, in \u001b[0;36mEndpoint._track_costs\u001b[0;34m(_Endpoint__func, with_endpoints, *args, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(callback)\n\u001b[1;32m    580\u001b[0m \u001b[38;5;66;03m# Call the function.\u001b[39;00m\n\u001b[0;32m--> 581\u001b[0m result: T \u001b[38;5;241m=\u001b[39m \u001b[43m__func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;66;03m# Return result and only the callbacks created here. Outer thunks might\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;66;03m# return others.\u001b[39;00m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result, callbacks\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trulens_eval/feedback/feedback.py:478\u001b[0m, in \u001b[0;36mFeedback.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeedback definition needs an implementation to call.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trulens_eval/feedback/provider/base.py:288\u001b[0m, in \u001b[0;36mLLMProvider.context_relevance\u001b[0;34m(self, question, context, temperature)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontext_relevance\u001b[39m(\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m, question: \u001b[38;5;28mstr\u001b[39m, context: \u001b[38;5;28mstr\u001b[39m, temperature: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    262\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m    Uses chat completion model. A function that completes a template to\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    check the relevance of the context to the question.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m        float: A value between 0.0 (not relevant) and 1.0 (relevant).\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCONTEXT_RELEVANCE_SYSTEM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCONTEXT_RELEVANCE_USER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trulens_eval/feedback/provider/base.py:161\u001b[0m, in \u001b[0;36mLLMProvider.generate_score\u001b[0;34m(self, system_prompt, user_prompt, normalize, temperature)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_prompt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     llm_messages\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_prompt})\n\u001b[0;32m--> 161\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_in_pace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_chat_completion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mod_generated_utils\u001b[38;5;241m.\u001b[39mre_0_10_rating(response) \u001b[38;5;241m/\u001b[39m normalize\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trulens_eval/feedback/provider/endpoint/base.py:313\u001b[0m, in \u001b[0;36mEndpoint.run_in_pace\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m             sleep(retry_delay)\n\u001b[1;32m    311\u001b[0m             retry_delay \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 313\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEndpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m request failed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretries\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m time(s): \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, errors)))\n\u001b[1;32m    316\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Endpoint litellm request failed 4 time(s): \n\tlitellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff174205db0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n\tlitellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff174224610>: Failed to establish a new connection: [Errno 111] Connection refused'))\n\tlitellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff174226ce0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n\tlitellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff17423d3f0>: Failed to establish a new connection: [Errno 111] Connection refused'))"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tru_recorder = TruLlama(filtered_query_engine,\n",
    "    app_id='LlamaIndex_App1_Filtered',\n",
    "    feedbacks=[f_answer_relevance, f_context_relevance, f_groundedness])\n",
    "\n",
    "with tru_recorder as recording:\n",
    "    llm_response = filtered_query_engine.query(\"What did the author do growing up?\")\n",
    "\n",
    "display(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7240f6d3-c278-436a-9085-f8fdda025bea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
